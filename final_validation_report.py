"""
æœ€çµ‚é©—è­‰å ±å‘Šç”Ÿæˆå™¨

æ•´åˆæ‰€æœ‰æ¨¡æ“¬æ•¸æ“šé©—è­‰æ¸¬è©¦çµæœï¼Œç”Ÿæˆå®Œæ•´çš„é©—è­‰å ±å‘Šå’Œæ”¹é€²å»ºè­°
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import json
import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, List, Any
from pathlib import Path
import glob

from src.utils.logger import get_component_logger

logger = get_component_logger("FinalValidationReport")


class FinalValidationReportGenerator:
    """æœ€çµ‚é©—è­‰å ±å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.report_data = {}
        self.recommendations = []
        
    def load_all_test_reports(self) -> Dict[str, Any]:
        """è¼‰å…¥æ‰€æœ‰æ¸¬è©¦å ±å‘Š"""
        logger.info("è¼‰å…¥æ‰€æœ‰æ¸¬è©¦å ±å‘Š...")
        
        reports = {}
        
        # æŸ¥æ‰¾æ‰€æœ‰å ±å‘Šæ–‡ä»¶
        report_files = {
            'simulation_validation': glob.glob('simulation_validation_report_*.json'),
            'boundary_condition': glob.glob('boundary_condition_test_report_*.json'),
            'consistency_verification': glob.glob('consistency_verification_report_*.json')
        }
        
        for report_type, files in report_files.items():
            if files:
                # é¸æ“‡æœ€æ–°çš„å ±å‘Š
                latest_file = max(files, key=os.path.getctime)
                try:
                    with open(latest_file, 'r', encoding='utf-8') as f:
                        reports[report_type] = json.load(f)
                    logger.info(f"è¼‰å…¥ {report_type} å ±å‘Šï¼š{latest_file}")
                except Exception as e:
                    logger.error(f"è¼‰å…¥ {report_type} å ±å‘Šå¤±æ•—ï¼š{e}")
                    reports[report_type] = None
            else:
                logger.warning(f"æœªæ‰¾åˆ° {report_type} å ±å‘Š")
                reports[report_type] = None
        
        return reports
    
    def analyze_overall_performance(self, reports: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææ•´é«”æ€§èƒ½"""
        logger.info("åˆ†ææ•´é«”æ€§èƒ½...")
        
        performance_summary = {
            'overall_score': 0.0,
            'category_scores': {},
            'critical_issues': [],
            'warnings': [],
            'strengths': []
        }
        
        # æ•¸æ“šå“è³ªåˆ†æ
        if reports['simulation_validation']:
            data_quality = reports['simulation_validation']['data_quality_metrics']
            performance_summary['category_scores']['data_quality'] = data_quality['overall_score']
            
            # è­˜åˆ¥é—œéµå•é¡Œ
            if data_quality['price_jump_rate'] < 0.8:
                performance_summary['critical_issues'].append('åƒ¹æ ¼è·³èºç‡éé«˜ï¼Œéœ€è¦èª¿æ•´åƒ¹æ ¼ç”Ÿæˆé‚è¼¯')
            
            if data_quality['volatility_accuracy'] < 0.5:
                performance_summary['critical_issues'].append('æ³¢å‹•ç‡åå·®éå¤§ï¼Œéœ€è¦æ ¡æ­£æ³¢å‹•ç‡åƒæ•¸')
            
            if data_quality['correlation_reasonability'] > 0.8:
                performance_summary['strengths'].append('è‚¡å‚µç›¸é—œæ€§åˆç†')
            
            if data_quality['yield_stability'] > 0.9:
                performance_summary['strengths'].append('å‚µåˆ¸æ®–åˆ©ç‡ç©©å®šæ€§è‰¯å¥½')
        
        # è¨ˆç®—æº–ç¢ºæ€§åˆ†æ
        if reports['simulation_validation']:
            calc_accuracy = reports['simulation_validation']['calculation_accuracy_metrics']
            performance_summary['category_scores']['calculation_accuracy'] = calc_accuracy['overall_score']
            
            if calc_accuracy['formula_verification_rate'] > 0.95:
                performance_summary['strengths'].append('å…¬å¼é©—è­‰ç‡å„ªç§€')
            
            if calc_accuracy['precision_maintenance_rate'] < 0.7:
                performance_summary['warnings'].append('ç²¾åº¦ä¿æŒéœ€è¦æ”¹é€²')
        
        # é‚Šç•Œæ¢ä»¶æ¸¬è©¦åˆ†æ
        if reports['boundary_condition']:
            boundary_score = reports['boundary_condition']['pass_rate']
            performance_summary['category_scores']['boundary_conditions'] = boundary_score
            
            if boundary_score == 1.0:
                performance_summary['strengths'].append('é‚Šç•Œæ¢ä»¶æ¸¬è©¦å…¨éƒ¨é€šé')
            elif boundary_score < 0.8:
                performance_summary['critical_issues'].append('é‚Šç•Œæ¢ä»¶æ¸¬è©¦å­˜åœ¨å¤±æ•—é …ç›®')
        
        # ä¸€è‡´æ€§é©—è­‰åˆ†æ
        if reports['consistency_verification']:
            consistency_details = reports['consistency_verification']['test_details']
            avg_consistency = np.mean([test['score'] for test in consistency_details.values() if 'score' in test])
            performance_summary['category_scores']['consistency'] = avg_consistency
            
            if avg_consistency < 0.6:
                performance_summary['warnings'].append('ä¸€è‡´æ€§é©—è­‰éœ€è¦æ”¹é€²')
            
            # æª¢æŸ¥ç‰¹å®šä¸€è‡´æ€§å•é¡Œ
            if 'parameter_sensitivity' in consistency_details:
                if not consistency_details['parameter_sensitivity']['passed']:
                    performance_summary['warnings'].append('åƒæ•¸æ•æ„Ÿæ€§æ¸¬è©¦æœªé€šéï¼Œéœ€è¦æª¢æŸ¥åƒæ•¸å½±éŸ¿')
        
        # è¨ˆç®—æ•´é«”è©•åˆ†
        scores = list(performance_summary['category_scores'].values())
        if scores:
            performance_summary['overall_score'] = np.mean(scores)
        
        return performance_summary
    
    def generate_improvement_recommendations(self, reports: Dict[str, Any], performance: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæ”¹é€²å»ºè­°"""
        logger.info("ç”Ÿæˆæ”¹é€²å»ºè­°...")
        
        recommendations = []
        
        # åŸºæ–¼æ•¸æ“šå“è³ªçš„å»ºè­°
        if reports['simulation_validation']:
            data_quality = reports['simulation_validation']['data_quality_metrics']
            
            if data_quality['volatility_accuracy'] < 0.5:
                recommendations.append({
                    'priority': 'HIGH',
                    'category': 'æ•¸æ“šå“è³ª',
                    'issue': 'æ³¢å‹•ç‡åå·®éå¤§',
                    'current_value': f"{data_quality['volatility_accuracy']:.2f}",
                    'target_value': '> 0.8',
                    'specific_action': 'èª¿æ•´ stock_volatility åƒæ•¸å¾ 0.20 åˆ° 0.23ï¼Œä½¿å¯¦éš›æ³¢å‹•ç‡æ›´æ¥è¿‘è¨­å®šçš„ 25%',
                    'implementation': 'src/ui/results_display.py ç¬¬ 868 è¡Œ',
                    'expected_impact': 'æé«˜æ³¢å‹•ç‡åŒ¹é…åº¦è‡³ 80% ä»¥ä¸Š'
                })
            
            if data_quality['price_jump_rate'] < 1.0:
                recommendations.append({
                    'priority': 'MEDIUM',
                    'category': 'æ•¸æ“šå“è³ª',
                    'issue': 'åƒ¹æ ¼è·³èºæ§åˆ¶',
                    'current_value': f"{data_quality['price_jump_rate']:.2f}",
                    'target_value': '1.0',
                    'specific_action': 'å„ªåŒ–åƒ¹æ ¼è®ŠåŒ–é™åˆ¶é‚è¼¯ï¼Œç¢ºä¿æ¥µç«¯è®ŠåŒ–æ§åˆ¶åœ¨ 8% ä»¥å…§',
                    'implementation': 'src/ui/results_display.py ç¬¬ 925-930 è¡Œ',
                    'expected_impact': 'å®Œå…¨æ¶ˆé™¤ç•°å¸¸åƒ¹æ ¼è·³èº'
                })
        
        # åŸºæ–¼çµæœåˆç†æ€§çš„å»ºè­°
        if reports['simulation_validation']:
            result_reason = reports['simulation_validation']['result_reasonability_metrics']
            
            if result_reason['max_drawdown_reasonability'] < 0.3:
                recommendations.append({
                    'priority': 'HIGH',
                    'category': 'ç­–ç•¥åˆç†æ€§',
                    'issue': 'æœ€å¤§å›æ’¤ä¸è¶³',
                    'current_value': f"{result_reason['max_drawdown_reasonability']:.2f}",
                    'target_value': '> 0.7',
                    'specific_action': 'å¢åŠ å¸‚å ´ä¸‹è·Œæƒ…å¢ƒï¼Œç¢ºä¿ç­–ç•¥æœ‰ 5%-15% çš„çœŸå¯¦å›æ’¤é¢¨éšª',
                    'implementation': 'åœ¨æ¨¡æ“¬æ•¸æ“šä¸­åŠ å…¥é€±æœŸæ€§ä¸‹è·Œéšæ®µ',
                    'expected_impact': 'ä½¿ç­–ç•¥è¡¨ç¾æ›´ç¬¦åˆçœŸå¯¦å¸‚å ´é¢¨éšª'
                })
            
            if result_reason['long_term_growth_consistency'] < 0.2:
                recommendations.append({
                    'priority': 'HIGH',
                    'category': 'é•·æœŸä¸€è‡´æ€§',
                    'issue': 'æˆé•·ç‡åå·®éå¤§',
                    'current_value': f"{result_reason['long_term_growth_consistency']:.2f}",
                    'target_value': '> 0.7',
                    'specific_action': 'èª¿æ•´ stock_growth_rate å¾ 0.015 åˆ° 0.006ï¼Œä½¿å¹´åŒ–æˆé•·ç‡æ¥è¿‘è¨­å®šçš„ 2.4%',
                    'implementation': 'src/ui/results_display.py ç¬¬ 867 è¡Œ',
                    'expected_impact': 'é•·æœŸæˆé•·è»Œè·¡ç¬¦åˆé æœŸï¼Œåå·®æ§åˆ¶åœ¨ 20% ä»¥å…§'
                })
        
        # åŸºæ–¼ä¸€è‡´æ€§é©—è­‰çš„å»ºè­°
        if reports['consistency_verification']:
            consistency_details = reports['consistency_verification']['test_details']
            
            if 'frequency_consistency' in consistency_details and not consistency_details['frequency_consistency']['passed']:
                recommendations.append({
                    'priority': 'MEDIUM',
                    'category': 'è·¨é »ç‡ä¸€è‡´æ€§',
                    'issue': 'ä¸åŒæŠ•è³‡é »ç‡çµæœä¸ä¸€è‡´',
                    'current_value': f"{consistency_details['frequency_consistency']['score']:.2f}",
                    'target_value': '> 0.8',
                    'specific_action': 'ç¢ºä¿å¹´åŒ–æ”¶ç›Šç‡è¨ˆç®—åœ¨ä¸åŒé »ç‡ä¸‹ä¿æŒä¸€è‡´æ€§',
                    'implementation': 'æª¢æŸ¥ä¸¦çµ±ä¸€å„é »ç‡çš„è¤‡åˆ©è¨ˆç®—é‚è¼¯',
                    'expected_impact': 'è·¨é »ç‡æŠ•è³‡çµæœå…·æœ‰å¯æ¯”æ€§'
                })
        
        # é€šç”¨æ”¹é€²å»ºè­°
        recommendations.append({
            'priority': 'LOW',
            'category': 'ç³»çµ±å„ªåŒ–',
            'issue': 'ç²¾åº¦ä¿æŒ',
            'current_value': '0.5-0.6',
            'target_value': '> 0.9',
            'specific_action': 'çµ±ä¸€æ•¸å€¼ç²¾åº¦æ ¼å¼ï¼Œé™åˆ¶å°æ•¸ä½æ•¸åœ¨ 4 ä½ä»¥å…§',
            'implementation': 'åœ¨æ‰€æœ‰è¨ˆç®—çµæœè¼¸å‡ºå‰é€²è¡Œæ ¼å¼åŒ–',
            'expected_impact': 'æé«˜æ•¸æ“šå‘ˆç¾çš„å°ˆæ¥­æ€§å’Œä¸€è‡´æ€§'
        })
        
        # æŒ‰å„ªå…ˆç´šæ’åº
        priority_order = {'HIGH': 1, 'MEDIUM': 2, 'LOW': 3}
        recommendations.sort(key=lambda x: priority_order[x['priority']])
        
        return recommendations
    
    def generate_comprehensive_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆç¶œåˆå ±å‘Š"""
        logger.info("ç”Ÿæˆç¶œåˆé©—è­‰å ±å‘Š...")
        
        # è¼‰å…¥æ‰€æœ‰æ¸¬è©¦å ±å‘Š
        reports = self.load_all_test_reports()
        
        # åˆ†ææ•´é«”æ€§èƒ½
        performance = self.analyze_overall_performance(reports)
        
        # ç”Ÿæˆæ”¹é€²å»ºè­°
        recommendations = self.generate_improvement_recommendations(reports, performance)
        
        # æ§‹å»ºç¶œåˆå ±å‘Š
        comprehensive_report = {
            'report_metadata': {
                'generated_at': datetime.now().isoformat(),
                'validation_plan_version': '1.0',
                'total_test_categories': 3,
                'report_generator': 'FinalValidationReportGenerator'
            },
            'executive_summary': {
                'overall_score': performance['overall_score'],
                'overall_status': self._determine_overall_status(performance['overall_score']),
                'critical_issues_count': len(performance['critical_issues']),
                'warnings_count': len(performance['warnings']),
                'strengths_count': len(performance['strengths']),
                'high_priority_recommendations': len([r for r in recommendations if r['priority'] == 'HIGH'])
            },
            'detailed_performance': performance,
            'test_results_summary': self._summarize_test_results(reports),
            'improvement_recommendations': recommendations,
            'implementation_roadmap': self._create_implementation_roadmap(recommendations),
            'quality_metrics': self._calculate_quality_metrics(reports),
            'raw_reports': reports
        }
        
        return comprehensive_report
    
    def _determine_overall_status(self, score: float) -> str:
        """åˆ¤æ–·æ•´é«”ç‹€æ…‹"""
        if score >= 0.8:
            return 'EXCELLENT'
        elif score >= 0.7:
            return 'GOOD'
        elif score >= 0.6:
            return 'ACCEPTABLE'
        elif score >= 0.5:
            return 'NEEDS_IMPROVEMENT'
        else:
            return 'CRITICAL'
    
    def _summarize_test_results(self, reports: Dict[str, Any]) -> Dict[str, Any]:
        """ç¸½çµæ¸¬è©¦çµæœ"""
        summary = {}
        
        if reports['simulation_validation']:
            summary['simulation_validation'] = {
                'overall_score': reports['simulation_validation']['overall_score'],
                'data_quality_score': reports['simulation_validation']['data_quality_metrics']['overall_score'],
                'calculation_accuracy_score': reports['simulation_validation']['calculation_accuracy_metrics']['overall_score'],
                'result_reasonability_score': reports['simulation_validation']['result_reasonability_metrics']['overall_score']
            }
        
        if reports['boundary_condition']:
            summary['boundary_condition'] = {
                'pass_rate': reports['boundary_condition']['pass_rate'],
                'total_tests': reports['boundary_condition']['total_tests'],
                'passed_tests': reports['boundary_condition']['passed_tests']
            }
        
        if reports['consistency_verification']:
            summary['consistency_verification'] = {
                'pass_rate': reports['consistency_verification']['pass_rate'],
                'total_tests': reports['consistency_verification']['total_tests'],
                'passed_tests': reports['consistency_verification']['passed_tests']
            }
        
        return summary
    
    def _create_implementation_roadmap(self, recommendations: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
        """å‰µå»ºå¯¦æ–½è·¯ç·šåœ–"""
        roadmap = {
            'immediate_actions': [],  # é«˜å„ªå…ˆç´šï¼Œ1é€±å…§
            'short_term_goals': [],   # ä¸­å„ªå…ˆç´šï¼Œ1å€‹æœˆå…§
            'long_term_improvements': []  # ä½å„ªå…ˆç´šï¼Œ3å€‹æœˆå…§
        }
        
        for rec in recommendations:
            if rec['priority'] == 'HIGH':
                roadmap['immediate_actions'].append({
                    'action': rec['specific_action'],
                    'implementation': rec['implementation'],
                    'expected_impact': rec['expected_impact']
                })
            elif rec['priority'] == 'MEDIUM':
                roadmap['short_term_goals'].append({
                    'action': rec['specific_action'],
                    'implementation': rec['implementation'],
                    'expected_impact': rec['expected_impact']
                })
            else:
                roadmap['long_term_improvements'].append({
                    'action': rec['specific_action'],
                    'implementation': rec['implementation'],
                    'expected_impact': rec['expected_impact']
                })
        
        return roadmap
    
    def _calculate_quality_metrics(self, reports: Dict[str, Any]) -> Dict[str, float]:
        """è¨ˆç®—å“è³ªæŒ‡æ¨™"""
        metrics = {}
        
        if reports['simulation_validation']:
            sv = reports['simulation_validation']
            metrics.update({
                'data_completeness': 1.0 if sv['data_summary']['market_data_periods'] > 0 else 0.0,
                'calculation_reliability': sv['calculation_accuracy_metrics']['overall_score'],
                'result_credibility': sv['result_reasonability_metrics']['overall_score'],
                'data_consistency': sv['data_quality_metrics']['overall_score']
            })
        
        if reports['boundary_condition']:
            metrics['robustness'] = reports['boundary_condition']['pass_rate']
        
        if reports['consistency_verification']:
            metrics['stability'] = reports['consistency_verification']['pass_rate']
        
        return metrics
    
    def save_report(self, report: Dict[str, Any], filename: str = None) -> str:
        """ä¿å­˜å ±å‘Š"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"final_validation_report_{timestamp}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2, default=str)
            
            logger.info(f"æœ€çµ‚é©—è­‰å ±å‘Šå·²ä¿å­˜è‡³ï¼š{filename}")
            return filename
        except Exception as e:
            logger.error(f"ä¿å­˜å ±å‘Šå¤±æ•—ï¼š{e}")
            return ""
    
    def print_executive_summary(self, report: Dict[str, Any]):
        """åˆ—å°åŸ·è¡Œæ‘˜è¦"""
        summary = report['executive_summary']
        performance = report['detailed_performance']
        
        print("\n" + "="*80)
        print("æ¨¡æ“¬è³‡æ–™æŸ¥æ ¸ - æœ€çµ‚é©—è­‰å ±å‘Š")
        print("="*80)
        
        print(f"\nğŸ“Š æ•´é«”è©•ä¼°")
        print(f"  ç¸½é«”è©•åˆ†ï¼š{summary['overall_score']:.2f}/1.00")
        print(f"  æ•´é«”ç‹€æ…‹ï¼š{summary['overall_status']}")
        
        print(f"\nğŸ¯ å•é¡Œçµ±è¨ˆ")
        print(f"  é—œéµå•é¡Œï¼š{summary['critical_issues_count']} é …")
        print(f"  è­¦å‘Šäº‹é …ï¼š{summary['warnings_count']} é …")
        print(f"  å„ªå‹¢é …ç›®ï¼š{summary['strengths_count']} é …")
        print(f"  é«˜å„ªå…ˆç´šå»ºè­°ï¼š{summary['high_priority_recommendations']} é …")
        
        print(f"\nğŸ“ˆ åˆ†é¡è©•åˆ†")
        for category, score in performance['category_scores'].items():
            print(f"  {category}: {score:.2f}")
        
        if performance['critical_issues']:
            print(f"\nğŸš¨ é—œéµå•é¡Œ")
            for issue in performance['critical_issues']:
                print(f"  â€¢ {issue}")
        
        if performance['strengths']:
            print(f"\nâœ… å„ªå‹¢é …ç›®")
            for strength in performance['strengths']:
                print(f"  â€¢ {strength}")
        
        print(f"\nğŸ“‹ æ”¹é€²å»ºè­°æ‘˜è¦")
        recommendations = report['improvement_recommendations']
        for priority in ['HIGH', 'MEDIUM', 'LOW']:
            priority_recs = [r for r in recommendations if r['priority'] == priority]
            if priority_recs:
                print(f"  {priority} å„ªå…ˆç´šï¼š{len(priority_recs)} é …")
                for rec in priority_recs[:2]:  # åªé¡¯ç¤ºå‰2é …
                    print(f"    - {rec['issue']}: {rec['specific_action']}")


def main():
    """ä¸»å‡½æ•¸"""
    generator = FinalValidationReportGenerator()
    
    # ç”Ÿæˆç¶œåˆå ±å‘Š
    comprehensive_report = generator.generate_comprehensive_report()
    
    # åˆ—å°åŸ·è¡Œæ‘˜è¦
    generator.print_executive_summary(comprehensive_report)
    
    # ä¿å­˜å ±å‘Š
    filename = generator.save_report(comprehensive_report)
    
    print(f"\nğŸ’¾ å®Œæ•´å ±å‘Šå·²ä¿å­˜è‡³ï¼š{filename}")
    print("\n" + "="*80)
    print("âœ… æ¨¡æ“¬è³‡æ–™æŸ¥æ ¸å®Œæˆ")
    print("="*80)
    
    return comprehensive_report


if __name__ == "__main__":
    main() 